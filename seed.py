# -*- coding: utf-8 -*-
"""seed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAhW1O4gwOQCYaeOL_iO4lJb7wtndiQy
"""

import torch
from torchvision.transforms import v2
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision.io import decode_image
from pathlib import Path
device = torch.device('cuda')

PLANT_CLASSES = [
    'Black-grass',
    'Charlock',
    'Cleavers',
    'Common Chickweed',
    'Common wheat',
    'Fat Hen',
    'Loose Silky-bent',
    'Maize',
    'Scentless Mayweed',
    'Shepherds Purse',
    'Small-flowered Cranesbill',
    'Sugar beet'
]

class PlantDataset(Dataset):

    def __init__(self, rootDir, transform=None):
        self.rootDir = Path(rootDir)
        self.transforms = transform
        self.classes = PLANT_CLASSES
        self.classsToidx = {plant : idx for idx, plant in enumerate(PLANT_CLASSES)} #å°‡classStringè½‰æˆ index çš„ dict

        self.samples = []
        self._load_samples()

    def _load_samples(self):
        """è¼‰å…¥æŒ‡å®špathä¸­çš„è³‡æ–™ å°‡'è·¯å¾‘'å’Œlabelç´€éŒ„åˆ°samplesä¸­"""
        for class_name in self.classes:
            class_dir = self.rootDir / class_name
            if class_dir.exists():
                for img_path in class_dir.glob('*.png'):
                    self.samples.append((str(img_path), self.classsToidx[class_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        img_path, label = self.samples[index]
        try:
            image = decode_image(img_path, mode='RGB')
        except Exception as e:
            print(f"ç„¡æ³•è¼‰å…¥åœ–ç‰‡{img_path}:{e}")
        if self.transforms:
            image = self.transforms(image)
        return image, label

transform_train = v2.Compose([
    v2.Resize((256, 256)),
    v2.RandomResizedCrop(224), #resnet50æ¨™æº–è¼¸å…¥size
    v2.RandomHorizontalFlip(0.5),
    v2.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.2,
        hue=0.1
    ),

    v2.RandomRotation(15),
    v2.RandomGrayscale(p=0.1),

    v2.ToDtype(torch.float32, scale=True),

    v2.Normalize(
        mean=[0.485, 0.456, 0.406],#0~1 floatè¨ˆç®—çš„mean
        std=[0.229, 0.224, 0.225]
    )#ImageNet è³‡æ–™çš„mean è·Ÿ std
])

transform_val = v2.Compose([
    v2.Resize((224, 224)),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(
        mean=[0.485, 0.456, 0.406],#0~1 floatè¨ˆç®—çš„mean
        std=[0.229, 0.224, 0.225]
    )#ImageNet è³‡æ–™çš„mean è·Ÿ std
])

def create_dataloaders_with_split(data_dir, batch_size=64, num_workers=2, val_split=0.1):

    train_dir = data_dir / 'train'

    train_dataset = PlantDataset(rootDir=train_dir, transform=transform_train)
    val_dataset = PlantDataset(rootDir=train_dir, transform=transform_val)

    total_size = len(train_dataset)
    val_size = int(total_size * val_split)
    train_size = total_size - val_size

    train_indices, val_indices = random_split(
        #ç‚ºäº†å¥—ä¸åŒçš„transfroms ç”¨idxåˆ†å‰²
        range(total_size),
        [train_size, val_size],
        generator=torch.Generator().manual_seed(30)
    )

    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(val_dataset, val_indices)

    train_dataset.dataset.transforms = transform_train
    val_dataset.dataset.transforms = transform_val

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        persistent_workers=True,
        prefetch_factor=4,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        persistent_workers=True,
        prefetch_factor=4,
        pin_memory=True,
        drop_last=False,
    )

    return train_loader, val_loader

class TestDataset(Dataset):

    def __init__(self, rootDir, transform=None):
        self.rootDir = Path(rootDir)
        self.transforms = transform
        self.samples = []
        self._load_samples()

    def _load_samples(self):
        """è¼‰å…¥æ¸¬è©¦è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰åœ–ç‰‡"""
        for img_path in self.rootDir.glob('*.png'):
            self.samples.append(str(img_path))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        img_path = self.samples[index]
        try:
            image = decode_image(img_path, mode='RGB')
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥åœ–ç‰‡ {img_path}: {e}")

        if self.transforms:
            image = self.transforms(image)

        filename = Path(img_path).name
        return image, filename

def create_test_dataloader(data_dir, batch_size=64, num_workers=2):
    """å»ºç«‹æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨"""

    test_dir = data_dir / 'test'

    test_dataset = TestDataset(
        rootDir=test_dir,
        transform=transform_val
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        persistent_workers=True,
        prefetch_factor=2,
        pin_memory=True,
        drop_last=False,
    )

    return test_loader

import torch.nn as nn
import torchvision.models as models
from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR

def create_resnet50_model(num_classes=12):
    """å»ºç«‹ ResNet-50 æ¨¡å‹"""

    # ğŸ¯ è¼‰å…¥é è¨“ç·´æ¨¡å‹
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model

def create_resnet50_model_freeze(num_classes=12):
    """å»ºç«‹ ResNet-50 æ¨¡å‹"""

    # ğŸ¯ è¼‰å…¥é è¨“ç·´æ¨¡å‹
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

    for name, param in model.named_parameters():
        if any(layer in name for layer in ['conv1', 'bn1', 'layer1', 'layer2']):
            param.requires_grad = False
        else:
            param.requires_grad = True

    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model

def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """è¨“ç·´ä¸€å€‹ epoch"""

    model.train()  # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼
    running_loss = 0.0
    correct = 0
    total = 0

    print(f"ğŸš‚ Epoch {epoch} è¨“ç·´ä¸­...")

    for batch_idx, (images, labels) in enumerate(train_loader):
        # ğŸ“¦ è³‡æ–™ç§»åˆ° GPU
        images, labels = images.to(device), labels.to(device)

        # ğŸ”„ å‰å‘å‚³æ’­
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)

        # ğŸ”™ åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # ğŸ“Š çµ±è¨ˆ
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        # ğŸ“ˆ é¡¯ç¤ºé€²åº¦
        if batch_idx % 10 == 0:
            print(f"  æ‰¹æ¬¡ {batch_idx}/{len(train_loader)}: "
                  f"Loss={loss.item():.4f}, "
                  f"Acc={100.*correct/total:.2f}%")

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

def validate_one_epoch(model, val_loader, criterion, device, epoch):
    """é©—è­‰ä¸€å€‹ epoch"""

    model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
    running_loss = 0.0
    correct = 0
    total = 0

    print(f"âœ… Epoch {epoch} é©—è­‰ä¸­...")

    with torch.no_grad():  # ä¸è¨ˆç®—æ¢¯åº¦
        for images, labels in val_loader:
            # ğŸ“¦ è³‡æ–™ç§»åˆ° GPU
            images, labels = images.to(device), labels.to(device)

            # ğŸ”„ å‰å‘å‚³æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)

            # ğŸ“Š çµ±è¨ˆ
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

import numpy as np
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True,
                 save_path='best_model.pth', verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.save_path = save_path
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.inf
        self.best_weights = None

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'ğŸ”” EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.restore_best_weights and self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                    if self.verbose:
                        print('ğŸ”„ Restored best weights')
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        if self.verbose:
            print(f'ğŸ“‰ Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')

        # ä¿å­˜æœ€ä½³æ¬Šé‡åˆ°è¨˜æ†¶é«”
        self.best_weights = model.state_dict().copy()
        self.val_loss_min = val_loss

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                device, num_epochs=20, save_path='best_model.pth', early_stopping_patience=10):
    """å®Œæ•´è¨“ç·´æµç¨‹"""

    print("ğŸš€ é–‹å§‹è¨“ç·´ ResNet-50")
    print("="*50)
    print(f"ğŸ“Š è¨“ç·´é›†æ‰¹æ¬¡æ•¸: {len(train_loader)}")
    print(f"ğŸ“Š é©—è­‰é›†æ‰¹æ¬¡æ•¸: {len(val_loader)}")
    print(f"â° ç¸½è¨“ç·´è¼ªæ•¸: {num_epochs}")
    print("="*50)

    # ğŸ“ˆ è¨˜éŒ„è¨“ç·´æ­·å²
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }

    early_stopping = EarlyStopping(
        patience=early_stopping_patience,
        min_delta=0.001,
        restore_best_weights=True,
        save_path=save_path,
        verbose=True
    )

    best_val_acc = 0.0

    for epoch in range(1, num_epochs + 1):
        print(f"\nğŸ”„ Epoch {epoch}/{num_epochs}")
        print("-" * 30)

        # ğŸš‚ è¨“ç·´éšæ®µ
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )

        # âœ… é©—è­‰éšæ®µ
        val_loss, val_acc = validate_one_epoch(
            model, val_loader, criterion, device, epoch
        )

        # ğŸ“… æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']

        # ğŸ“Š è¨˜éŒ„çµæœ
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # ğŸ“ˆ é¡¯ç¤ºçµæœ
        print(f"\nğŸ“Š Epoch {epoch} çµæœ:")
        print(f"  ğŸš‚ è¨“ç·´ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  âœ… é©—è­‰ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"  ğŸ“… å­¸ç¿’ç‡: {current_lr:.6f}")

        # ğŸ’¾ å„²å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'history': history
            }, save_path)
            print(f"  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼é©—è­‰æº–ç¢ºç‡: {val_acc:.2f}%")

        #åšearly stopping
        if early_stopping_patience > 0:
            early_stopping(val_loss, model)

            if early_stopping.early_stop:
                print(f"\nğŸ›‘ Early stopping triggered at epoch {epoch}!")
                print(f"ğŸ¯ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
                break

    print(f"\nğŸ‰ è¨“ç·´å®Œæˆï¼")
    print(f"ğŸ† æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
    print(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {save_path}")

    return history

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

def get_predictions(model, data_loader, device):
    """ç²å–æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„é æ¸¬çµæœ"""
    model.eval()
    all_preds = []
    all_labels = []

    print("ğŸ” æ­£åœ¨ç”Ÿæˆé æ¸¬çµæœ...")

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_preds), np.array(all_labels)

import pandas as pd
from tqdm import tqdm

def predict_test_dataset(model, test_loader, device, class_names, save_path='predictions.csv'):
    """å°æ¸¬è©¦è³‡æ–™é›†é€²è¡Œé æ¸¬ä¸¦è¼¸å‡º CSV"""

    print("ğŸ” é–‹å§‹é æ¸¬æ¸¬è©¦è³‡æ–™é›†:")
    print("="*40)
    print(f"ğŸ“Š æ¸¬è©¦æ‰¹æ¬¡æ•¸: {len(test_loader)}")
    print(f"ğŸ“ é æ¸¬çµæœå°‡ä¿å­˜è‡³: {save_path}")
    print("="*40)

    model.eval()

    # å„²å­˜é æ¸¬çµæœ
    predictions_list = []
    filenames_list = []
    confidences_list = []

    with torch.no_grad():
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        for batch_idx, (images, filenames) in enumerate(tqdm(test_loader, desc="ğŸš€ é æ¸¬ä¸­")):
            images = images.to(device)

            # å‰å‘å‚³æ’­
            outputs = model(images)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            confidences, predicted = torch.max(probabilities, 1)

            # è½‰æ›ç‚º CPU ä¸¦æ”¶é›†çµæœ
            predicted = predicted.cpu().numpy()
            confidences = confidences.cpu().numpy()

            # å°‡é æ¸¬é¡åˆ¥ç´¢å¼•è½‰æ›ç‚ºé¡åˆ¥åç¨±
            for i, (pred_idx, conf, filename) in enumerate(zip(predicted, confidences, filenames)):
                species_name = class_names[pred_idx]

                predictions_list.append(species_name)
                filenames_list.append(filename)
                confidences_list.append(conf)

    # å»ºç«‹ DataFrame
    results_df = pd.DataFrame({
        'file': filenames_list,
        'species': predictions_list,
        'confidence': confidences_list
    })

    # æŒ‰æª”åæ’åº
    results_df = results_df.sort_values('file').reset_index(drop=True)

    # ä¿å­˜ CSVï¼ˆåªåŒ…å« file å’Œ species æ¬„ä½ï¼Œç¬¦åˆä½ çš„æ ¼å¼è¦æ±‚ï¼‰
    final_df = results_df[['file', 'species']]
    final_df.to_csv(save_path, index=False)

    print(f"\nâœ… é æ¸¬å®Œæˆï¼")
    print(f"ğŸ“Š ç¸½é æ¸¬æ¨£æœ¬æ•¸: {len(results_df)}")
    print(f"ğŸ“ çµæœå·²ä¿å­˜è‡³: {save_path}")

    # é¡¯ç¤ºé æ¸¬çµ±è¨ˆ
    print(f"\nğŸ“Š é æ¸¬çµ±è¨ˆ:")
    species_counts = results_df['species'].value_counts()
    for species, count in species_counts.items():
        percentage = (count / len(results_df)) * 100
        print(f"   {species}: {count} å¼µ ({percentage:.1f}%)")

    # é¡¯ç¤ºä¿¡å¿ƒåº¦çµ±è¨ˆ
    print(f"\nğŸ¯ é æ¸¬ä¿¡å¿ƒåº¦çµ±è¨ˆ:")
    print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {results_df['confidence'].mean():.3f}")
    print(f"   æœ€é«˜ä¿¡å¿ƒåº¦: {results_df['confidence'].max():.3f}")
    print(f"   æœ€ä½ä¿¡å¿ƒåº¦: {results_df['confidence'].min():.3f}")
    print(f"   é«˜ä¿¡å¿ƒåº¦ (>0.9): {(results_df['confidence'] > 0.9).sum()} å¼µ")
    print(f"   ä½ä¿¡å¿ƒåº¦ (<0.5): {(results_df['confidence'] < 0.5).sum()} å¼µ")

    # é¡¯ç¤ºå‰å¹¾å€‹é æ¸¬çµæœä½œç‚ºç¯„ä¾‹
    print(f"\nğŸ“‹ å‰ 10 å€‹é æ¸¬çµæœ:")
    print(final_df.head(10).to_string(index=False))

    return results_df, final_df

# ğŸš€ åŸ·è¡Œæ¸¬è©¦é æ¸¬
def run_test_prediction():
    """åŸ·è¡Œå®Œæ•´çš„æ¸¬è©¦é æ¸¬æµç¨‹"""

    print("ğŸš€ é–‹å§‹æ¸¬è©¦é æ¸¬æµç¨‹:")
    print("="*50)

    # 1. è¼‰å…¥æœ€ä½³æ¨¡å‹
    model_path = 'resnet50_plant_classifier.pth'
    print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {model_path}")

    try:
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ! (æœ€ä½³é©—è­‰æº–ç¢ºç‡: {checkpoint['best_val_acc']:.2f}%)")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None


    if len(test_loader) == 0:
        print("âŒ æ¸¬è©¦è³‡æ–™å¤¾ç‚ºç©ºæˆ–ç„¡æ³•è®€å–")
        return None

    print(f"ğŸ“Š æ¸¬è©¦è³‡æ–™è¼‰å…¥å®Œæˆ: {len(test_loader.dataset)} å¼µåœ–ç‰‡")

    # 3. é€²è¡Œé æ¸¬
    results_df, final_df = predict_test_dataset(
        model=model,
        test_loader=test_loader,
        device=device,
        class_names=PLANT_CLASSES,
        save_path='test_predictions.csv'
    )

    return results_df, final_df

# â­ ä¸»ç¨‹åºä¿è­· - è§£æ±º Windows å¤šé€²ç¨‹å•é¡Œ
if __name__ == '__main__':
    # ğŸ”§ è¨­ç½®å¤šé€²ç¨‹å•Ÿå‹•æ–¹æ³• (Windows å¿…éœ€)
    torch.multiprocessing.set_start_method('spawn', force=True)
    
    # ğŸ“‚ å»ºç«‹è³‡æ–™è¼‰å…¥å™¨
    data_dir = Path('./data')
    train_loader, val_loader = create_dataloaders_with_split(data_dir, batch_size=64, num_workers=4)
    test_loader = create_test_dataloader(data_dir, batch_size=64, num_workers=4)

    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # ğŸ¤– å»ºç«‹æ¨¡å‹
    model = create_resnet50_model(num_classes=len(PLANT_CLASSES))
    #model = create_resnet50_model_freeze(num_classes=len(PLANT_CLASSES))
    model = model.to(device)

    # ğŸ“Š é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"ğŸ“ˆ ç¸½åƒæ•¸é‡: {total_params:,}")
    print(f"ğŸ¯ å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")

    # ğŸ¯ è¨­ç½®è¨“ç·´åƒæ•¸
    class_weights = torch.ones(12)  # å‡è¨­æœ‰12å€‹é¡åˆ¥
    class_weights[0] = 2.0  # å¢åŠ label 0çš„æ¬Šé‡
    class_weights[6] = 1.5  # å¢åŠ label 6çš„æ¬Šé‡
    class_weights = class_weights.to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    optimizer = Adam(
        model.parameters(),
        lr=0.001,
        weight_decay=0.0001  # L2 æ­£å‰‡åŒ–
    )

    scheduler = StepLR(
        optimizer,
        step_size=7,    # æ¯ 7 å€‹ epoch é™ä½å­¸ç¿’ç‡
        gamma=0.1       # å­¸ç¿’ç‡ä¹˜ä»¥ 0.1
    )

    # ğŸš€ é–‹å§‹è¨“ç·´ï¼
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        num_epochs=50,
        save_path='resnet50_plant_classifier.pth',
        early_stopping_patience=10
        )

    # ğŸ“Š ç¹ªè£½è¨“ç·´æ›²ç·š
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title('Accuracy Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    # ğŸ“Š æ··æ·†çŸ©é™£
    y_pred, y_true = get_predictions(model, val_loader, device)
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # ğŸ¯ åŸ·è¡Œæ¸¬è©¦é æ¸¬
    prediction_results = run_test_prediction()
