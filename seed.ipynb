{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a8b98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.io import decode_image\n",
    "from pathlib import Path\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "611069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANT_CLASSES = [\n",
    "    'Black-grass',\n",
    "    'Charlock', \n",
    "    'Cleavers',\n",
    "    'Common Chickweed',\n",
    "    'Common wheat',\n",
    "    'Fat Hen',\n",
    "    'Loose Silky-bent',\n",
    "    'Maize',\n",
    "    'Scentless Mayweed',\n",
    "    'Shepherds Purse',\n",
    "    'Small-flowered Cranesbill',\n",
    "    'Sugar beet'\n",
    "]\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "\n",
    "    def __init__(self, rootDir, transform=None):\n",
    "        self.rootDir = Path(rootDir)\n",
    "        self.transforms = transform\n",
    "        self.classes = PLANT_CLASSES\n",
    "        self.classsToidx = {plant : idx for idx, plant in enumerate(PLANT_CLASSES)} #å°‡classStringè½‰æˆ index çš„ dict\n",
    "\n",
    "        self.samples = []\n",
    "        self._load_samples()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"è¼‰å…¥æŒ‡å®špathä¸­çš„è³‡æ–™ å°‡'è·¯å¾‘'å’Œlabelç´€éŒ„åˆ°samplesä¸­\"\"\"\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.rootDir / class_name\n",
    "            if class_dir.exists():\n",
    "                for img_path in class_dir.glob('*.png'):\n",
    "                    self.samples.append((str(img_path), self.classsToidx[class_name]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.samples[index]\n",
    "        try:\n",
    "            image = decode_image(img_path, mode='RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"ç„¡æ³•è¼‰å…¥åœ–ç‰‡{img_path}:{e}\")\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c1a3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.RandomResizedCrop(224), #resnet50æ¨™æº–è¼¸å…¥size\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomGrayscale(p=0.1),\n",
    "\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    v2.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],#0~1 floatè¨ˆç®—çš„mean\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )#ImageNet è³‡æ–™çš„mean è·Ÿ std\n",
    "])\n",
    "\n",
    "transform_val = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],#0~1 floatè¨ˆç®—çš„mean\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )#ImageNet è³‡æ–™çš„mean è·Ÿ std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0918351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders_with_split(data_dir, batch_size=64, num_workers=2, val_split=0.2):\n",
    "\n",
    "    train_dir = data_dir / 'train'\n",
    "\n",
    "    full_dataset = PlantDataset(\n",
    "        rootDir=train_dir,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    total_size = len(full_dataset)\n",
    "    val_size = int(total_size * val_split)\n",
    "    train_size = total_size - val_size\n",
    "\n",
    "    train_indices, val_indices = random_split(\n",
    "        #ç‚ºäº†å¥—ä¸åŒçš„transfroms ç”¨idxåˆ†å‰²\n",
    "        range(total_size), \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    \n",
    "    train_dataset.dataset.transforms = transform_train\n",
    "    val_dataset.dataset.transforms = transform_val\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "310118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, rootDir, transform=None):\n",
    "        self.rootDir = Path(rootDir)\n",
    "        self.transforms = transform\n",
    "        self.samples = []\n",
    "        self._load_samples()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"è¼‰å…¥æ¸¬è©¦è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰åœ–ç‰‡\"\"\"\n",
    "        for img_path in self.rootDir.glob('*.png'):\n",
    "            self.samples.append(str(img_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.samples[index]\n",
    "        try:\n",
    "            image = decode_image(img_path, mode='RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç„¡æ³•è¼‰å…¥åœ–ç‰‡ {img_path}: {e}\") \n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        filename = Path(img_path).name\n",
    "        return image, filename\n",
    "\n",
    "def create_test_dataloader(data_dir, batch_size=64, num_workers=2):\n",
    "    \"\"\"å»ºç«‹æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨\"\"\"\n",
    "    \n",
    "    test_dir = data_dir / 'test'\n",
    "    \n",
    "    test_dataset = TestDataset(\n",
    "        rootDir=test_dir,\n",
    "        transform=transform_val\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6764abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data')\n",
    "train_loader, val_loader = create_dataloaders_with_split(data_dir, batch_size=32, num_workers=0)\n",
    "test_loader = create_test_dataloader(data_dir, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a14e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2084adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ ç¸½åƒæ•¸é‡: 23,532,620\n",
      "ğŸ¯ å¯è¨“ç·´åƒæ•¸: 23,532,620\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def create_resnet50_model(num_classes=12):\n",
    "    \"\"\"å»ºç«‹ ResNet-50 æ¨¡å‹\"\"\"\n",
    "    \n",
    "    # ğŸ¯ è¼‰å…¥é è¨“ç·´æ¨¡å‹\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_resnet50_model_freeze(num_classes=12):\n",
    "    \"\"\"å»ºç«‹ ResNet-50 æ¨¡å‹\"\"\"\n",
    "    \n",
    "    # ğŸ¯ è¼‰å…¥é è¨“ç·´æ¨¡å‹\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_resnet50_model(num_classes=len(PLANT_CLASSES))\n",
    "model = model.to(device)\n",
    "modelFreeze = create_resnet50_model_freeze(num_classes=len(PLANT_CLASSES))\n",
    "modelFreeze = modelFreeze.to(device)\n",
    "\n",
    "# ğŸ“Š é¡¯ç¤ºæ¨¡å‹è³‡è¨Š\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"ğŸ“ˆ ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "print(f\"ğŸ¯ å¯è¨“ç·´åƒæ•¸: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1c00abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0001  # L2 æ­£å‰‡åŒ–\n",
    ")\n",
    "    \n",
    "scheduler = StepLR(\n",
    "    optimizer,\n",
    "    step_size=7,    # æ¯ 7 å€‹ epoch é™ä½å­¸ç¿’ç‡\n",
    "    gamma=0.1       # å­¸ç¿’ç‡ä¹˜ä»¥ 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be7a9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"è¨“ç·´ä¸€å€‹ epoch\"\"\"\n",
    "    \n",
    "    model.train()  # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"ğŸš‚ Epoch {epoch} è¨“ç·´ä¸­...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # ğŸ“¦ è³‡æ–™ç§»åˆ° GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # ğŸ”„ å‰å‘å‚³æ’­\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # ğŸ”™ åå‘å‚³æ’­\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ğŸ“Š çµ±è¨ˆ\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # ğŸ“ˆ é¡¯ç¤ºé€²åº¦\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  æ‰¹æ¬¡ {batch_idx}/{len(train_loader)}: \"\n",
    "                  f\"Loss={loss.item():.4f}, \"\n",
    "                  f\"Acc={100.*correct/total:.2f}%\")\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch):\n",
    "    \"\"\"é©—è­‰ä¸€å€‹ epoch\"\"\"\n",
    "    \n",
    "    model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"âœ… Epoch {epoch} é©—è­‰ä¸­...\")\n",
    "    \n",
    "    with torch.no_grad():  # ä¸è¨ˆç®—æ¢¯åº¦\n",
    "        for images, labels in val_loader:\n",
    "            # ğŸ“¦ è³‡æ–™ç§»åˆ° GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # ğŸ”„ å‰å‘å‚³æ’­\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # ğŸ“Š çµ±è¨ˆ\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "072d7a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹è¨“ç·´ ResNet-50\n",
      "==================================================\n",
      "ğŸ“Š è¨“ç·´é›†æ‰¹æ¬¡æ•¸: 118\n",
      "ğŸ“Š é©—è­‰é›†æ‰¹æ¬¡æ•¸: 30\n",
      "â° ç¸½è¨“ç·´è¼ªæ•¸: 20\n",
      "==================================================\n",
      "\n",
      "ğŸ”„ Epoch 1/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 1 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=2.4780, Acc=6.25%\n",
      "  æ‰¹æ¬¡ 10/118: Loss=1.6328, Acc=38.35%\n",
      "  æ‰¹æ¬¡ 20/118: Loss=1.1615, Acc=51.49%\n",
      "  æ‰¹æ¬¡ 30/118: Loss=0.4183, Acc=58.06%\n",
      "  æ‰¹æ¬¡ 40/118: Loss=0.3005, Acc=62.88%\n",
      "  æ‰¹æ¬¡ 50/118: Loss=0.6914, Acc=65.75%\n",
      "  æ‰¹æ¬¡ 60/118: Loss=0.6175, Acc=68.14%\n",
      "  æ‰¹æ¬¡ 70/118: Loss=0.5082, Acc=70.29%\n",
      "  æ‰¹æ¬¡ 80/118: Loss=0.4150, Acc=72.03%\n",
      "  æ‰¹æ¬¡ 90/118: Loss=0.6999, Acc=73.18%\n",
      "  æ‰¹æ¬¡ 100/118: Loss=0.7601, Acc=74.01%\n",
      "  æ‰¹æ¬¡ 110/118: Loss=0.5192, Acc=74.77%\n",
      "âœ… Epoch 1 é©—è­‰ä¸­...\n",
      "\n",
      "ğŸ“Š Epoch 1 çµæœ:\n",
      "  ğŸš‚ è¨“ç·´ - Loss: 0.7641, Acc: 75.05%\n",
      "  âœ… é©—è­‰ - Loss: 0.5962, Acc: 82.32%\n",
      "  ğŸ“… å­¸ç¿’ç‡: 0.001000\n",
      "  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼é©—è­‰æº–ç¢ºç‡: 82.32%\n",
      "\n",
      "ğŸ”„ Epoch 2/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 2 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=0.2128, Acc=96.88%\n",
      "  æ‰¹æ¬¡ 10/118: Loss=0.3491, Acc=86.93%\n",
      "  æ‰¹æ¬¡ 20/118: Loss=0.1346, Acc=88.24%\n",
      "  æ‰¹æ¬¡ 30/118: Loss=0.1556, Acc=87.40%\n",
      "  æ‰¹æ¬¡ 40/118: Loss=0.3159, Acc=88.34%\n",
      "  æ‰¹æ¬¡ 50/118: Loss=0.1169, Acc=89.09%\n",
      "  æ‰¹æ¬¡ 60/118: Loss=0.2777, Acc=89.40%\n",
      "  æ‰¹æ¬¡ 70/118: Loss=0.2766, Acc=89.39%\n",
      "  æ‰¹æ¬¡ 80/118: Loss=0.2638, Acc=89.31%\n",
      "  æ‰¹æ¬¡ 90/118: Loss=0.4976, Acc=89.32%\n",
      "  æ‰¹æ¬¡ 100/118: Loss=0.3258, Acc=89.51%\n",
      "  æ‰¹æ¬¡ 110/118: Loss=0.3570, Acc=89.58%\n",
      "âœ… Epoch 2 é©—è­‰ä¸­...\n",
      "\n",
      "ğŸ“Š Epoch 2 çµæœ:\n",
      "  ğŸš‚ è¨“ç·´ - Loss: 0.2994, Acc: 89.67%\n",
      "  âœ… é©—è­‰ - Loss: 0.3062, Acc: 89.89%\n",
      "  ğŸ“… å­¸ç¿’ç‡: 0.001000\n",
      "  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼é©—è­‰æº–ç¢ºç‡: 89.89%\n",
      "\n",
      "ğŸ”„ Epoch 3/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 3 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=0.0807, Acc=96.88%\n",
      "  æ‰¹æ¬¡ 10/118: Loss=0.1068, Acc=93.18%\n",
      "  æ‰¹æ¬¡ 20/118: Loss=0.2319, Acc=92.71%\n",
      "  æ‰¹æ¬¡ 30/118: Loss=0.1649, Acc=93.75%\n",
      "  æ‰¹æ¬¡ 40/118: Loss=0.2197, Acc=94.51%\n",
      "  æ‰¹æ¬¡ 50/118: Loss=0.3854, Acc=94.49%\n",
      "  æ‰¹æ¬¡ 60/118: Loss=0.2218, Acc=94.36%\n",
      "  æ‰¹æ¬¡ 70/118: Loss=0.1461, Acc=93.88%\n",
      "  æ‰¹æ¬¡ 80/118: Loss=0.2159, Acc=93.98%\n",
      "  æ‰¹æ¬¡ 90/118: Loss=0.4038, Acc=93.51%\n",
      "  æ‰¹æ¬¡ 100/118: Loss=0.2200, Acc=93.41%\n",
      "  æ‰¹æ¬¡ 110/118: Loss=0.1577, Acc=93.47%\n",
      "âœ… Epoch 3 é©—è­‰ä¸­...\n",
      "\n",
      "ğŸ“Š Epoch 3 çµæœ:\n",
      "  ğŸš‚ è¨“ç·´ - Loss: 0.2067, Acc: 93.41%\n",
      "  âœ… é©—è­‰ - Loss: 0.3178, Acc: 88.84%\n",
      "  ğŸ“… å­¸ç¿’ç‡: 0.001000\n",
      "\n",
      "ğŸ”„ Epoch 4/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 4 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=0.2201, Acc=87.50%\n",
      "  æ‰¹æ¬¡ 10/118: Loss=0.0455, Acc=95.74%\n",
      "  æ‰¹æ¬¡ 20/118: Loss=0.0757, Acc=96.58%\n",
      "  æ‰¹æ¬¡ 30/118: Loss=0.0931, Acc=96.07%\n",
      "  æ‰¹æ¬¡ 40/118: Loss=0.0823, Acc=95.81%\n",
      "  æ‰¹æ¬¡ 50/118: Loss=0.3279, Acc=95.47%\n",
      "  æ‰¹æ¬¡ 60/118: Loss=0.1571, Acc=94.93%\n",
      "  æ‰¹æ¬¡ 70/118: Loss=0.1311, Acc=95.11%\n",
      "  æ‰¹æ¬¡ 80/118: Loss=0.1267, Acc=95.18%\n",
      "  æ‰¹æ¬¡ 90/118: Loss=0.1562, Acc=95.30%\n",
      "  æ‰¹æ¬¡ 100/118: Loss=0.2629, Acc=95.30%\n",
      "  æ‰¹æ¬¡ 110/118: Loss=0.0615, Acc=95.52%\n",
      "âœ… Epoch 4 é©—è­‰ä¸­...\n",
      "\n",
      "ğŸ“Š Epoch 4 çµæœ:\n",
      "  ğŸš‚ è¨“ç·´ - Loss: 0.1446, Acc: 95.50%\n",
      "  âœ… é©—è­‰ - Loss: 1.3897, Acc: 76.11%\n",
      "  ğŸ“… å­¸ç¿’ç‡: 0.001000\n",
      "\n",
      "ğŸ”„ Epoch 5/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 5 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=0.1061, Acc=93.75%\n",
      "  æ‰¹æ¬¡ 10/118: Loss=0.2153, Acc=93.75%\n",
      "  æ‰¹æ¬¡ 20/118: Loss=0.2976, Acc=93.75%\n",
      "  æ‰¹æ¬¡ 30/118: Loss=0.1673, Acc=93.75%\n",
      "  æ‰¹æ¬¡ 40/118: Loss=0.0847, Acc=93.29%\n",
      "  æ‰¹æ¬¡ 50/118: Loss=0.1466, Acc=93.20%\n",
      "  æ‰¹æ¬¡ 60/118: Loss=0.0301, Acc=93.49%\n",
      "  æ‰¹æ¬¡ 70/118: Loss=0.1947, Acc=93.27%\n",
      "  æ‰¹æ¬¡ 80/118: Loss=0.4251, Acc=93.40%\n",
      "  æ‰¹æ¬¡ 90/118: Loss=0.0696, Acc=93.51%\n",
      "  æ‰¹æ¬¡ 100/118: Loss=0.2916, Acc=93.90%\n",
      "  æ‰¹æ¬¡ 110/118: Loss=0.1182, Acc=93.98%\n",
      "âœ… Epoch 5 é©—è­‰ä¸­...\n",
      "\n",
      "ğŸ“Š Epoch 5 çµæœ:\n",
      "  ğŸš‚ è¨“ç·´ - Loss: 0.1760, Acc: 94.12%\n",
      "  âœ… é©—è­‰ - Loss: 0.4827, Acc: 85.26%\n",
      "  ğŸ“… å­¸ç¿’ç‡: 0.001000\n",
      "\n",
      "ğŸ”„ Epoch 6/20\n",
      "------------------------------\n",
      "ğŸš‚ Epoch 6 è¨“ç·´ä¸­...\n",
      "  æ‰¹æ¬¡ 0/118: Loss=0.0902, Acc=96.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# ğŸš€ é–‹å§‹è¨“ç·´ï¼\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m history = train_model(\n\u001b[32m     72\u001b[39m     model=model,\n\u001b[32m     73\u001b[39m     train_loader=train_loader,\n\u001b[32m     74\u001b[39m     val_loader=val_loader,\n\u001b[32m     75\u001b[39m     criterion=criterion,\n\u001b[32m     76\u001b[39m     optimizer=optimizer,\n\u001b[32m     77\u001b[39m     scheduler=scheduler,\n\u001b[32m     78\u001b[39m     device=device,\n\u001b[32m     79\u001b[39m     num_epochs=\u001b[32m20\u001b[39m,\n\u001b[32m     80\u001b[39m     save_path=\u001b[33m'\u001b[39m\u001b[33mresnet50_plant_classifier.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     81\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, save_path)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ğŸš‚ è¨“ç·´éšæ®µ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_loss, train_acc = train_one_epoch(\n\u001b[32m     28\u001b[39m     model, train_loader, criterion, optimizer, device, epoch\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# âœ… é©—è­‰éšæ®µ\u001b[39;00m\n\u001b[32m     32\u001b[39m val_loss, val_acc = validate_one_epoch(\n\u001b[32m     33\u001b[39m     model, val_loader, criterion, device, epoch\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epoch)\u001b[39m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ğŸ“Š çµ±è¨ˆ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m running_loss += loss.item()\n\u001b[32m     26\u001b[39m _, predicted = outputs.max(\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                device, num_epochs=20, save_path='best_model.pth'):\n",
    "    \"\"\"å®Œæ•´è¨“ç·´æµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ é–‹å§‹è¨“ç·´ ResNet-50\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ğŸ“Š è¨“ç·´é›†æ‰¹æ¬¡æ•¸: {len(train_loader)}\")\n",
    "    print(f\"ğŸ“Š é©—è­‰é›†æ‰¹æ¬¡æ•¸: {len(val_loader)}\")\n",
    "    print(f\"â° ç¸½è¨“ç·´è¼ªæ•¸: {num_epochs}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ğŸ“ˆ è¨˜éŒ„è¨“ç·´æ­·å²\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nğŸ”„ Epoch {epoch}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # ğŸš‚ è¨“ç·´éšæ®µ\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # âœ… é©—è­‰éšæ®µ\n",
    "        val_loss, val_acc = validate_one_epoch(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        # ğŸ“… æ›´æ–°å­¸ç¿’ç‡\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # ğŸ“Š è¨˜éŒ„çµæœ\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # ğŸ“ˆ é¡¯ç¤ºçµæœ\n",
    "        print(f\"\\nğŸ“Š Epoch {epoch} çµæœ:\")\n",
    "        print(f\"  ğŸš‚ è¨“ç·´ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  âœ… é©—è­‰ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  ğŸ“… å­¸ç¿’ç‡: {current_lr:.6f}\")\n",
    "        \n",
    "        # ğŸ’¾ å„²å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, save_path)\n",
    "            print(f\"  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼é©—è­‰æº–ç¢ºç‡: {val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è¨“ç·´å®Œæˆï¼\")\n",
    "    print(f\"ğŸ† æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%\")\n",
    "    print(f\"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {save_path}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# ğŸš€ é–‹å§‹è¨“ç·´ï¼\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    save_path='resnet50_plant_classifier.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"ç¹ªè£½è¨“ç·´æ­·å²åœ–è¡¨\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š ç¹ªè£½è¨“ç·´æ­·å²:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # è¨­ç½®ä¸­æ–‡å­—é«”å’Œé¢¨æ ¼\n",
    "    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # å‰µå»º 2x2 å­åœ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸŒ± æ¤ç‰©åˆ†é¡æ¨¡å‹è¨“ç·´çµæœ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # ğŸ“‰ Loss æ›²ç·š\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-o', label='è¨“ç·´ Loss', linewidth=2, markersize=4)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-s', label='é©—è­‰ Loss', linewidth=2, markersize=4)\n",
    "    axes[0, 0].set_title('ğŸ“‰ Loss è®ŠåŒ–æ›²ç·š', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ğŸ“ˆ æº–ç¢ºç‡æ›²ç·š\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-o', label='è¨“ç·´æº–ç¢ºç‡', linewidth=2, markersize=4)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-s', label='é©—è­‰æº–ç¢ºç‡', linewidth=2, markersize=4)\n",
    "    axes[0, 1].set_title('ğŸ“ˆ æº–ç¢ºç‡è®ŠåŒ–æ›²ç·š', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('æº–ç¢ºç‡ (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ğŸ“Š æœ€çµ‚å¹¾å€‹ epoch çš„è©³ç´°æ¯”è¼ƒ\n",
    "    last_n = min(5, len(epochs))\n",
    "    last_epochs = epochs[-last_n:]\n",
    "    \n",
    "    axes[1, 0].bar([f'E{e}' for e in last_epochs], \n",
    "                   [history['train_acc'][e-1] for e in last_epochs], \n",
    "                   alpha=0.7, color='skyblue', label='è¨“ç·´')\n",
    "    axes[1, 0].bar([f'E{e}' for e in last_epochs], \n",
    "                   [history['val_acc'][e-1] for e in last_epochs], \n",
    "                   alpha=0.7, color='lightcoral', label='é©—è­‰')\n",
    "    axes[1, 0].set_title(f'ğŸ“Š æœ€å¾Œ {last_n} å€‹ Epoch æº–ç¢ºç‡æ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('æº–ç¢ºç‡ (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ğŸ“ˆ å­¸ç¿’æ›²ç·šåˆ†æ\n",
    "    train_val_gap = [abs(t - v) for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "    axes[1, 1].plot(epochs, train_val_gap, 'g-^', label='è¨“ç·´-é©—è­‰å·®è·', linewidth=2, markersize=4)\n",
    "    axes[1, 1].axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='ç†æƒ³å·®è· (5%)')\n",
    "    axes[1, 1].set_title('ğŸ“ˆ éæ“¬åˆåˆ†æ', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('æº–ç¢ºç‡å·®è· (%)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ğŸ“Š æ‰“å°çµ±è¨ˆä¿¡æ¯\n",
    "    print(f\"ğŸ“Š è¨“ç·´çµ±è¨ˆ:\")\n",
    "    print(f\"   æœ€ä½³è¨“ç·´æº–ç¢ºç‡: {max(history['train_acc']):.2f}%\")\n",
    "    print(f\"   æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history['val_acc']):.2f}%\")\n",
    "    print(f\"   æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history['train_acc'][-1]:.2f}%\")\n",
    "    print(f\"   æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history['val_acc'][-1]:.2f}%\")\n",
    "    print(f\"   æœ€çµ‚éæ“¬åˆç¨‹åº¦: {abs(history['train_acc'][-1] - history['val_acc'][-1]):.2f}%\")\n",
    "\n",
    "# ğŸ¨ ç¹ªè£½è¨“ç·´æ­·å²\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6705b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹æ¸¬è©¦é æ¸¬æµç¨‹:\n",
      "==================================================\n",
      "ğŸ“¥ è¼‰å…¥æ¨¡å‹: resnet50_plant_classifier.pth\n",
      "âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ! (æœ€ä½³é©—è­‰æº–ç¢ºç‡: 89.89%)\n",
      "ğŸ“Š æ¸¬è©¦è³‡æ–™è¼‰å…¥å®Œæˆ: 794 å¼µåœ–ç‰‡\n",
      "ğŸ” é–‹å§‹é æ¸¬æ¸¬è©¦è³‡æ–™é›†:\n",
      "========================================\n",
      "ğŸ“Š æ¸¬è©¦æ‰¹æ¬¡æ•¸: 25\n",
      "ğŸ“ é æ¸¬çµæœå°‡ä¿å­˜è‡³: test_predictions.csv\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ é æ¸¬ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… é æ¸¬å®Œæˆï¼\n",
      "ğŸ“Š ç¸½é æ¸¬æ¨£æœ¬æ•¸: 794\n",
      "ğŸ“ çµæœå·²ä¿å­˜è‡³: test_predictions.csv\n",
      "\n",
      "ğŸ“Š é æ¸¬çµ±è¨ˆ:\n",
      "   Loose Silky-bent: 145 å¼µ (18.3%)\n",
      "   Common Chickweed: 99 å¼µ (12.5%)\n",
      "   Small-flowered Cranesbill: 85 å¼µ (10.7%)\n",
      "   Scentless Mayweed: 82 å¼µ (10.3%)\n",
      "   Sugar beet: 72 å¼µ (9.1%)\n",
      "   Fat Hen: 66 å¼µ (8.3%)\n",
      "   Charlock: 64 å¼µ (8.1%)\n",
      "   Cleavers: 48 å¼µ (6.0%)\n",
      "   Shepherds Purse: 45 å¼µ (5.7%)\n",
      "   Common wheat: 38 å¼µ (4.8%)\n",
      "   Maize: 36 å¼µ (4.5%)\n",
      "   Black-grass: 14 å¼µ (1.8%)\n",
      "\n",
      "ğŸ¯ é æ¸¬ä¿¡å¿ƒåº¦çµ±è¨ˆ:\n",
      "   å¹³å‡ä¿¡å¿ƒåº¦: 0.910\n",
      "   æœ€é«˜ä¿¡å¿ƒåº¦: 1.000\n",
      "   æœ€ä½ä¿¡å¿ƒåº¦: 0.310\n",
      "   é«˜ä¿¡å¿ƒåº¦ (>0.9): 588 å¼µ\n",
      "   ä½ä¿¡å¿ƒåº¦ (<0.5): 17 å¼µ\n",
      "\n",
      "ğŸ“‹ å‰ 10 å€‹é æ¸¬çµæœ:\n",
      "         file                   species\n",
      "0021e90e4.png Small-flowered Cranesbill\n",
      "003d61042.png                   Fat Hen\n",
      "007b3da8b.png                Sugar beet\n",
      "0086a6340.png          Common Chickweed\n",
      "00c47e980.png                Sugar beet\n",
      "00d090cde.png          Loose Silky-bent\n",
      "00ef713a8.png          Common Chickweed\n",
      "01291174f.png                   Fat Hen\n",
      "026716f9b.png          Loose Silky-bent\n",
      "02cfeb38d.png          Loose Silky-bent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_test_dataset(model, test_loader, device, class_names, save_path='predictions.csv'):\n",
    "    \"\"\"å°æ¸¬è©¦è³‡æ–™é›†é€²è¡Œé æ¸¬ä¸¦è¼¸å‡º CSV\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” é–‹å§‹é æ¸¬æ¸¬è©¦è³‡æ–™é›†:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"ğŸ“Š æ¸¬è©¦æ‰¹æ¬¡æ•¸: {len(test_loader)}\")\n",
    "    print(f\"ğŸ“ é æ¸¬çµæœå°‡ä¿å­˜è‡³: {save_path}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # å„²å­˜é æ¸¬çµæœ\n",
    "    predictions_list = []\n",
    "    filenames_list = []\n",
    "    confidences_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "        for batch_idx, (images, filenames) in enumerate(tqdm(test_loader, desc=\"ğŸš€ é æ¸¬ä¸­\")):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # å‰å‘å‚³æ’­\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidences, predicted = torch.max(probabilities, 1)\n",
    "            \n",
    "            # è½‰æ›ç‚º CPU ä¸¦æ”¶é›†çµæœ\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            confidences = confidences.cpu().numpy()\n",
    "            \n",
    "            # å°‡é æ¸¬é¡åˆ¥ç´¢å¼•è½‰æ›ç‚ºé¡åˆ¥åç¨±\n",
    "            for i, (pred_idx, conf, filename) in enumerate(zip(predicted, confidences, filenames)):\n",
    "                species_name = class_names[pred_idx]\n",
    "                \n",
    "                predictions_list.append(species_name)\n",
    "                filenames_list.append(filename)\n",
    "                confidences_list.append(conf)\n",
    "    \n",
    "    # å»ºç«‹ DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'file': filenames_list,\n",
    "        'species': predictions_list,\n",
    "        'confidence': confidences_list\n",
    "    })\n",
    "    \n",
    "    # æŒ‰æª”åæ’åº\n",
    "    results_df = results_df.sort_values('file').reset_index(drop=True)\n",
    "    \n",
    "    # ä¿å­˜ CSVï¼ˆåªåŒ…å« file å’Œ species æ¬„ä½ï¼Œç¬¦åˆä½ çš„æ ¼å¼è¦æ±‚ï¼‰\n",
    "    final_df = results_df[['file', 'species']]\n",
    "    final_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… é æ¸¬å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š ç¸½é æ¸¬æ¨£æœ¬æ•¸: {len(results_df)}\")\n",
    "    print(f\"ğŸ“ çµæœå·²ä¿å­˜è‡³: {save_path}\")\n",
    "    \n",
    "    # é¡¯ç¤ºé æ¸¬çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“Š é æ¸¬çµ±è¨ˆ:\")\n",
    "    species_counts = results_df['species'].value_counts()\n",
    "    for species, count in species_counts.items():\n",
    "        percentage = (count / len(results_df)) * 100\n",
    "        print(f\"   {species}: {count} å¼µ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # é¡¯ç¤ºä¿¡å¿ƒåº¦çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ¯ é æ¸¬ä¿¡å¿ƒåº¦çµ±è¨ˆ:\")\n",
    "    print(f\"   å¹³å‡ä¿¡å¿ƒåº¦: {results_df['confidence'].mean():.3f}\")\n",
    "    print(f\"   æœ€é«˜ä¿¡å¿ƒåº¦: {results_df['confidence'].max():.3f}\")\n",
    "    print(f\"   æœ€ä½ä¿¡å¿ƒåº¦: {results_df['confidence'].min():.3f}\")\n",
    "    print(f\"   é«˜ä¿¡å¿ƒåº¦ (>0.9): {(results_df['confidence'] > 0.9).sum()} å¼µ\")\n",
    "    print(f\"   ä½ä¿¡å¿ƒåº¦ (<0.5): {(results_df['confidence'] < 0.5).sum()} å¼µ\")\n",
    "    \n",
    "    # é¡¯ç¤ºå‰å¹¾å€‹é æ¸¬çµæœä½œç‚ºç¯„ä¾‹\n",
    "    print(f\"\\nğŸ“‹ å‰ 10 å€‹é æ¸¬çµæœ:\")\n",
    "    print(final_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return results_df, final_df\n",
    "\n",
    "# ğŸš€ åŸ·è¡Œæ¸¬è©¦é æ¸¬\n",
    "def run_test_prediction():\n",
    "    \"\"\"åŸ·è¡Œå®Œæ•´çš„æ¸¬è©¦é æ¸¬æµç¨‹\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ é–‹å§‹æ¸¬è©¦é æ¸¬æµç¨‹:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    model_path = 'resnet50_plant_classifier.pth'\n",
    "    print(f\"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ! (æœ€ä½³é©—è­‰æº–ç¢ºç‡: {checkpoint['best_val_acc']:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    if len(test_loader) == 0:\n",
    "        print(\"âŒ æ¸¬è©¦è³‡æ–™å¤¾ç‚ºç©ºæˆ–ç„¡æ³•è®€å–\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“Š æ¸¬è©¦è³‡æ–™è¼‰å…¥å®Œæˆ: {len(test_loader.dataset)} å¼µåœ–ç‰‡\")\n",
    "    \n",
    "    # 3. é€²è¡Œé æ¸¬\n",
    "    results_df, final_df = predict_test_dataset(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        class_names=PLANT_CLASSES,\n",
    "        save_path='test_predictions.csv'\n",
    "    )\n",
    "    \n",
    "    return results_df, final_df\n",
    "\n",
    "# ğŸ¯ åŸ·è¡Œé æ¸¬\n",
    "prediction_results = run_test_prediction()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
